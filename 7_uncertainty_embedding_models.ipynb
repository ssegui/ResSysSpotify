{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install edward\n",
    "#!pip install pathos\n",
    "#!pip install more_itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import edward as ed\n",
    "from tensorflow.python.training.adam import AdamOptimizer\n",
    "import time, os\n",
    "from math import sqrt\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from random import shuffle\n",
    "import collections\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from pathos.multiprocessing import Pool, cpu_count\n",
    "from more_itertools import chunked\n",
    "from typing import List, Callable, Union, Any\n",
    "from math import ceil\n",
    "from itertools import chain\n",
    "import logging\n",
    "from random import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "import edward as ed\n",
    "from edward.models import Normal, Bernoulli\n",
    "from tensorflow.contrib.tensorboard.plugins import projector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_name = 'uncertain_fits/fit' + time.strftime(\"%y_%m_%d_%H_%M_%S\")\n",
    "in_file = 'MPD_line_sentence/playlists_ntitle_tracks_sentences_id_final'\n",
    "context_emb_file = 'models/wv_model_mincount5_shuffle_size300_MPD'\n",
    "ns = 10\n",
    "K = 300\n",
    "cs = 20\n",
    "mb = 5000\n",
    "n_epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(dir_name)\n",
    "sess = ed.get_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "    \"\"\"Extract the first file enclosed in a zip file as a list of words\"\"\"\n",
    "    songs_and_tracks = np.load(filename)\n",
    "    logging.getLogger('logging_songscuncert').debug('number of loaded playists:'+str(len(songs_and_tracks)))\n",
    "    return songs_and_tracks\n",
    "\n",
    "\n",
    "def flatten_list(listoflists):\n",
    "    return list(chain.from_iterable(listoflists))\n",
    "\n",
    "\n",
    "def get_optimal():\n",
    "    x = np.linspace(0, 1, 100)\n",
    "    a = 1\n",
    "    optimals = []\n",
    "    for b in range(7, 17, 2):\n",
    "        optimal = np.power(x, a) * np.power((1 - x), b) + 1e-3\n",
    "        optimal = optimal / np.sum(optimal)\n",
    "        optimals.append(optimal)\n",
    "    return optimals\n",
    "\n",
    "\n",
    "def is_goog_embedding(sigmas):\n",
    "    threshold = 1e-3\n",
    "    optimals = get_optimal()\n",
    "    hist = plt.hist(sigmas, bins=100, color='green', label='sigma values')\n",
    "    distr = (hist[0] + 1e-5) / np.sum(hist[0])\n",
    "    distance = 0\n",
    "    for optimal in optimals:\n",
    "        distance += -np.sum(optimal * np.log(distr / optimal))\n",
    "    distance = distance / len(optimals)\n",
    "    return distance < threshold\n",
    "\n",
    "\n",
    "def process_play_list_constructor(neg_samples:int, dictionary:dict, context_size:int, sampling_table:dict):\n",
    "    \"\"\"Generate a function that will clean and tokenize text.\"\"\"\n",
    "    def process_play_list(play_lists):\n",
    "        samples = []\n",
    "        dictionary_keys = list(dictionary.keys())\n",
    "        try:\n",
    "            for play_list in play_lists:\n",
    "                if sampling_table[play_list[0]] < random.random():\n",
    "                    songs = play_list[1]\n",
    "                    shuffle(songs)\n",
    "                    for song in songs[:context_size]:\n",
    "                        if song not in dictionary:\n",
    "                            song = 'UNK'\n",
    "                        samples.append((int(play_list[0]), dictionary[song], 1))\n",
    "                    for i in range(neg_samples):\n",
    "                        random_neg_sample = random.randint(0, len(dictionary) - 1)\n",
    "                        samples.append((int(play_list[0]), dictionary[dictionary_keys[random_neg_sample]], 0))\n",
    "        except Exception as e:\n",
    "            logging.getLogger('logging_songscuncert').error('error '+e)\n",
    "        return samples\n",
    "\n",
    "    return process_play_list\n",
    "\n",
    "\n",
    "def apply_parallel(func: Callable,\n",
    "                   data: List[Any],\n",
    "                   cpu_cores: int = None) -> List[Any]:\n",
    "    \"\"\"\n",
    "    Apply function to list of elements.\n",
    "\n",
    "    Automatically determines the chunk size.\n",
    "    \"\"\"\n",
    "    if not cpu_cores:\n",
    "        cpu_cores = cpu_count()\n",
    "\n",
    "    try:\n",
    "        chunk_size = ceil(len(data) / cpu_cores)\n",
    "        pool = Pool(cpu_cores)\n",
    "        transformed_data = pool.map(func, chunked(data, chunk_size), chunksize=1)\n",
    "    finally:\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "        return transformed_data\n",
    "\n",
    "\n",
    "def variable_summaries(summary_name, var):\n",
    "    with tf.name_scope(summary_name):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.summary.scalar('mean', mean)\n",
    "        with tf.name_scope('stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "        tf.summary.scalar('stddev', stddev)\n",
    "        tf.summary.scalar('max', tf.reduce_max(var))\n",
    "        tf.summary.scalar('min', tf.reduce_min(var))\n",
    "\n",
    "\n",
    "def get_logger():\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "\n",
    "    # create logger\n",
    "    logger = logging.getLogger(\"logging_songscuncert\")\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "\n",
    "    # create console handler and set level to debug\n",
    "    ch = logging.StreamHandler()\n",
    "    ch.setLevel(logging.DEBUG)\n",
    "\n",
    "    # create formatter\n",
    "    formatter = logging.Formatter(\"%(asctime)s;%(levelname)s;%(message)s\")\n",
    "\n",
    "    # add formatter to ch\n",
    "    ch.setFormatter(formatter)\n",
    "\n",
    "    # add ch to logger\n",
    "    logger.addHandler(ch)\n",
    "    return logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class bayessian_bern_emb_data():\n",
    "    def __init__(self, input_file, target_emb_file, context_emb_file, ns, K, cs, dir_name):\n",
    "        self.ns = ns\n",
    "        self.K = K\n",
    "        self.cs = cs\n",
    "        self.dir_name = dir_name\n",
    "        if target_emb_file:\n",
    "            self.load_target_embeddings(target_emb_file)\n",
    "        else:\n",
    "            self.pretreained_target_embeddings = None\n",
    "        self.load_context_embeddings(context_emb_file)\n",
    "        songs_and_tracks = read_data(input_file)\n",
    "        self.build_dataset(songs_and_tracks)\n",
    "        #self.batch = self.batch_generator()\n",
    "        self.N = len(self.playlists)\n",
    "\n",
    "    def parallel_process_text(self, data):\n",
    "        \"\"\"Apply cleaner -> tokenizer.\"\"\"\n",
    "        process_text = process_play_list_constructor(self.ns, self.dictionary, self.cs, self.sampling_table)\n",
    "        return flatten_list(apply_parallel(process_text, data))\n",
    "\n",
    "    def build_dataset(self, songs_and_tracks):\n",
    "        raw_playlists, raw_songs = zip(*songs_and_tracks)\n",
    "        count_playlists = collections.Counter(raw_playlists)\n",
    "        self.L_target = len(count_playlists.keys())\n",
    "        self.build_sampling_table(count_playlists)\n",
    "        self.samples = self.parallel_process_text(songs_and_tracks)\n",
    "        shuffle(self.samples)\n",
    "        playlists, songs, labels = zip(*self.samples)\n",
    "        self.playlists = np.array(list(playlists))\n",
    "        self.songs = np.array(list(songs))\n",
    "        self.labels = np.array(list(labels))\n",
    "\n",
    "    def batch_generator(self, n_minibatch):\n",
    "        batch_size = n_minibatch\n",
    "        data_target = self.playlists\n",
    "        data_context = self.songs\n",
    "        data_labels = self.labels\n",
    "        while True:\n",
    "            if data_target.shape[0] < batch_size:\n",
    "                data_target = np.hstack([data_target, self.playlists])\n",
    "                data_context = np.hstack([data_context, self.songs])\n",
    "                data_labels = np.hstack([data_labels, self.labels])\n",
    "                if data_target.shape[0] < batch_size:\n",
    "                    continue\n",
    "            play_lists = data_target[:batch_size]\n",
    "            songs = data_context[:batch_size]\n",
    "            labels = data_labels[:batch_size]\n",
    "            data_target = data_target[batch_size:]\n",
    "            data_context = data_context[batch_size:]\n",
    "            data_labels = data_labels[batch_size:]\n",
    "            yield play_lists, songs, labels\n",
    "\n",
    "    def feed(self, n_minibatch, target_placeholder, context_placeholder, labels_placeholder,\n",
    "             ones_placeholder, zeros_placeholder, shuffling = False):\n",
    "        play_lists, songs, labels = self.batch.__next__()\n",
    "        if shuffling:\n",
    "            labels = np.random.permutation(labels)\n",
    "        return {target_placeholder: play_lists,\n",
    "                context_placeholder: songs,\n",
    "                labels_placeholder: labels,\n",
    "                ones_placeholder: np.ones((n_minibatch), dtype=np.int32),\n",
    "                zeros_placeholder: np.zeros((n_minibatch), dtype=np.int32)\n",
    "                }\n",
    "\n",
    "    def load_target_embeddings(self, emb_file):\n",
    "        w2v_model = Word2Vec.load(emb_file)\n",
    "        target_embeddings = []\n",
    "        for elem in range(len(w2v_model.wv.vectors)):\n",
    "            target_embeddings.append(w2v_model.wv.word_vec(str(elem)))\n",
    "        self.pretreained_target_embeddings = np.array(target_embeddings)\n",
    "\n",
    "    def load_context_embeddings(self, emb_file):\n",
    "        w2v_model = Word2Vec.load(emb_file)\n",
    "        vocabulary = w2v_model.wv.vocab\n",
    "        self.dictionary = {'UNK': 0}\n",
    "        for song in vocabulary:\n",
    "            self.dictionary[song] = vocabulary[song].index + 1\n",
    "        self.L_context = len(self.dictionary)\n",
    "        self.pretreained_context_embeddings = np.zeros((1, self.K), dtype=np.float32).tolist()\n",
    "        self.pretreained_context_embeddings.extend(w2v_model.wv.vectors)\n",
    "\n",
    "    def build_sampling_table(self, count_playlists):\n",
    "        sampling_factor = 1e-3\n",
    "        sampling_table = dict()\n",
    "        total_occurrences = sum(count_playlists.values())\n",
    "        for playlist in count_playlists:\n",
    "            playlist_frequency = (1. * count_playlists[playlist]) / total_occurrences\n",
    "            sampling_table[playlist] = max(0., ((playlist_frequency - sampling_factor) / playlist_frequency) - sqrt(\n",
    "                sampling_factor / playlist_frequency))\n",
    "        self.sampling_table = sampling_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = bayessian_bern_emb_data(in_file, None, context_emb_file, ns, K, cs, dir_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class bayesian_emb_model():\n",
    "    def __init__(self, d, n_minibatch, sess, logdir):\n",
    "        self.K = d.K\n",
    "        self.sess = sess\n",
    "        self.logdir = logdir\n",
    "\n",
    "        with tf.name_scope('model'):\n",
    "            # Data Placeholder\n",
    "            with tf.name_scope('input'):\n",
    "                self.target_placeholder = tf.placeholder(tf.int32)\n",
    "                self.context_placeholder = tf.placeholder(tf.int32)\n",
    "                self.labels_placeholder = tf.placeholder(tf.int32, shape=[n_minibatch])\n",
    "                self.ones_placeholder = tf.placeholder(tf.int32)\n",
    "                self.zeros_placeholder = tf.placeholder(tf.int32)\n",
    "\n",
    "            # Index Masks\n",
    "            with tf.name_scope('priors'):\n",
    "                self.U = Normal(loc=tf.zeros((d.L_target, self.K), dtype=tf.float32),\n",
    "                                scale=tf.ones((d.L_target, self.K), dtype=tf.float32))\n",
    "                self.V = Normal(loc=tf.zeros((d.L_context, self.K), dtype=tf.float32),\n",
    "                                scale=tf.ones((d.L_context, self.K), dtype=tf.float32))\n",
    "\n",
    "        with tf.name_scope('natural_param'):\n",
    "            # Taget and Context Indices\n",
    "            with tf.name_scope('target_word'):\n",
    "                pos_indexes = tf.where(\n",
    "                    tf.equal(self.labels_placeholder, tf.ones(self.labels_placeholder.shape, dtype=tf.int32)))\n",
    "                pos_words = tf.gather(self.target_placeholder, pos_indexes)\n",
    "                self.p_rhos = tf.nn.embedding_lookup(self.U, pos_words)\n",
    "                pos_contexts = tf.gather(self.context_placeholder, pos_indexes)\n",
    "                self.pos_ctx_alpha = tf.nn.embedding_lookup(self.V, pos_contexts)\n",
    "\n",
    "            with tf.name_scope('negative_samples'):\n",
    "                neg_indexes = tf.where(\n",
    "                    tf.equal(self.labels_placeholder, tf.zeros(self.labels_placeholder.shape, dtype=tf.int32)))\n",
    "                neg_words = tf.gather(self.target_placeholder, neg_indexes)\n",
    "                self.n_rho = tf.nn.embedding_lookup(self.U, neg_words)\n",
    "                neg_contexts = tf.gather(self.context_placeholder, neg_indexes)\n",
    "                self.neg_ctx_alpha = tf.nn.embedding_lookup(self.V, neg_contexts)\n",
    "\n",
    "            # Natural parameter\n",
    "            self.p_eta = tf.reduce_sum(tf.multiply(self.p_rhos, self.pos_ctx_alpha), -1)\n",
    "            self.n_eta = tf.reduce_sum(tf.multiply(self.n_rho, self.neg_ctx_alpha), -1)\n",
    "\n",
    "        self.y_pos = Bernoulli(logits=self.p_eta)\n",
    "        self.y_neg = Bernoulli(logits=self.n_eta)\n",
    "\n",
    "        # INFERENCE\n",
    "        self.sigU = tf.nn.softplus(\n",
    "            tf.matmul(tf.get_variable(\"sigU\", shape=(d.L_target, 1), initializer=tf.ones_initializer()), tf.ones([1, self.K])),\n",
    "            name=\"sigmasU\")\n",
    "        self.sigV = tf.nn.softplus(\n",
    "            tf.matmul(tf.get_variable(\"sigV\", shape=(d.L_context, 1), initializer=tf.ones_initializer()), tf.ones([1, self.K])),\n",
    "            name=\"sigmasV\")\n",
    "        self.locU = tf.get_variable(\"qU/loc\", [d.L_target, self.K], initializer=tf.zeros_initializer())\n",
    "        #self.locV = tf.get_variable(\"qV/loc\", [d.L_context, self.K], initializer=tf.zeros_initializer())\n",
    "\n",
    "        if d.pretreained_target_embeddings is not None:\n",
    "            self.qU = Normal(loc=d.pretreained_target_embeddings, scale=self.sigU)\n",
    "        else:\n",
    "            self.qU = Normal(loc=self.locU, scale=self.sigU)\n",
    "        self.qV = Normal(loc=d.pretreained_context_embeddings, scale=self.sigV)\n",
    "\n",
    "        self.inference = ed.KLqp({self.U: self.qU, self.V: self.qV},\n",
    "                                 data={self.y_pos: self.ones_placeholder,\n",
    "                                       self.y_neg: self.zeros_placeholder\n",
    "                                       })\n",
    "        with self.sess.as_default():\n",
    "            tf.global_variables_initializer().run()\n",
    "        self.summaries = tf.summary.merge_all()\n",
    "        self.train_writer = tf.summary.FileWriter(self.logdir, self.sess.graph)\n",
    "        self.saver = tf.train.Saver()\n",
    "        config = projector.ProjectorConfig()\n",
    "\n",
    "        alpha = config.embeddings.add()\n",
    "        alpha.tensor_name = 'qU/loc'\n",
    "        alpha.metadata_path = '../vocab_alpha.tsv'\n",
    "        rho = config.embeddings.add()\n",
    "        rho.tensor_name = 'qV/loc'\n",
    "        rho.metadata_path = '../vocab_rho.tsv'\n",
    "        projector.visualize_embeddings(self.train_writer, config)\n",
    "\n",
    "    def dump(self, fname, data):\n",
    "        with self.sess.as_default():\n",
    "            dat = {'rhos': self.qU.loc.eval(),\n",
    "                   'alpha': self.qV.loc.eval(),\n",
    "                   'sigma_rhos': self.sigU.eval()[:, 0],\n",
    "                   'sigma_alphas': self.sigV.eval()[:, 0]}\n",
    "            pickle.dump(dat, open(fname, \"wb+\"))\n",
    "\n",
    "    def build_words_list(self, labels, list_length):\n",
    "        if len(labels) < list_length:\n",
    "            empty_list = [''] * (list_length - len(labels))\n",
    "            labels.extend(empty_list)\n",
    "        return labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.batch = d.batch_generator(mb)\n",
    "m = bayesian_emb_model(d, mb, sess, dir_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_iters():\n",
    "    n_batches = len(d.playlists) / mb\n",
    "    if len(d.playlists) % mb > 0:\n",
    "        n_batches += 1\n",
    "    return int(n_batches) * n_epochs, int(n_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_n_iters' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-05eca2177e9c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# TRAINING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msigmas_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mn_iters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_batches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_n_iters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m m.inference.initialize(n_samples=1, n_iter=n_iters, logdir=m.logdir,\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_n_iters' is not defined"
     ]
    }
   ],
   "source": [
    "# TRAINING\n",
    "sigmas_list = list()\n",
    "n_iters, n_batches = get_n_iters()\n",
    "\n",
    "m.inference.initialize(n_samples=1, n_iter=n_iters, logdir=m.logdir,\n",
    "                       scale={m.y_pos: n_batches, m.y_neg: n_batches / ns},\n",
    "                       kl_scaling={m.y_pos: n_batches, m.y_neg: n_batches / ns},\n",
    "                       optimizer=AdamOptimizer(learning_rate=0.001)\n",
    "                       )\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "for i in range(m.inference.n_iter):\n",
    "    info_dict = m.inference.update(feed_dict=d.feed(mb, m.target_placeholder,\n",
    "                                                    m.context_placeholder,\n",
    "                                                    m.labels_placeholder,\n",
    "                                                    m.ones_placeholder,\n",
    "                                                    m.zeros_placeholder,\n",
    "                                                    True))\n",
    "    m.inference.print_progress(info_dict)\n",
    "    if i % 10000 == 0:\n",
    "        m.saver.save(sess, os.path.join(m.logdir, \"model.ckpt\"), i)\n",
    "        sigmas = m.sigU.eval()[:, 0]\n",
    "        sigmas_list.append(sigmas)\n",
    "        pickle.dump(sigmas_list, open(dir_name + \"/sigmas.dat\", \"wb+\"))\n",
    "        if is_goog_embedding(sigmas):\n",
    "            break\n",
    "m.saver.save(sess, os.path.join(m.logdir, \"model.ckpt\"), i)\n",
    "m.dump(dir_name + \"/variational.dat\", d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
